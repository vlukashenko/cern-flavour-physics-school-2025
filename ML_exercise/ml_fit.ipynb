{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f5fbf3",
   "metadata": {},
   "source": [
    "Example of maximum-likelihood fit with iminuit version 2.\n",
    "pdf is a mixture of Gaussian (signal) and exponential (background),\n",
    "truncated in [xMin,xMax].\n",
    "G. Cowan / RHUL Physics / December 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ac5fe-e891-4cfc-9d4b-51989182cab5",
   "metadata": {},
   "source": [
    "We first will start with importing all the necessary packages. The exercise is based on python implementation of Minuit package. We will use it to perform an unbinned MLE fit to the generated mass-spectrum with one peak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbac3347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iminuit version: 2.31.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import truncexpon #truncated exponential in [a,b]\n",
    "from scipy.stats import truncnorm  #truncated normal distribution in [a,b]\n",
    "from scipy.stats import chi2\n",
    "import iminuit\n",
    "from iminuit import Minuit\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import container\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "print(\"iminuit version:\", iminuit.__version__)  # need 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a49a7-2891-4861-8cad-15922b0359ff",
   "metadata": {},
   "source": [
    "Now, here we will upload the dataset generated for you beforehand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e46b84",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/dataset.csv\", header=None)\n",
    "xData = df.values  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8bb3a-b164-402d-84f0-4886480af663",
   "metadata": {},
   "source": [
    "The model we will use is rather simple. The signal is modelled with a single Gaussian, while the background is modelled with exponential. The total PDF is defined as following :\n",
    "$$f(m) = \\theta \\cdot G(m; \\mu, \\sigma) + (1 - \\theta)\\cdot Exp(m; \\xi)$$\n",
    "where $\\theta$ is signal fraction and $\\xi$ is exponential power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfb756",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def f(x, par):\n",
    "    theta   = par[0]\n",
    "    mu      = par[1]\n",
    "    sigma   = par[2]\n",
    "    xi      = par[3]\n",
    "    fs = stats.truncnorm.pdf(x, a=(xMin-mu)/sigma, b=(xMax-mu)/sigma, loc=mu, scale=sigma)\n",
    "    fb = stats.truncexpon.pdf(x, b=(xMax-xMin)/xi, loc=xMin, scale=xi)\n",
    "    return theta*fs + (1-theta)*fb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff81451-9269-408a-b499-8ebfe400ab28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 1
   },
   "source": [
    "We will now have to implement the nLL ourselves. Note, that Minuit expects the nLL python function to depend ONLY on the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c2793-9eb6-4b9a-b255-249575805bb6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def negLogL(par):\n",
    "    pdf = f(xData, par)\n",
    "    return -np.sum(np.log(pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950cd3d5-035e-4849-98b8-7464f6416082",
   "metadata": {},
   "source": [
    "# Task 1 \n",
    "\n",
    "Configure the iminuit fit. For this we need to setup the 2 POI $\\theta$ and $\\xi$ and 2 nuisance parameters $\\mu$ and $\\sigma$. \n",
    "1. We configure initial values\n",
    "2. We configure initial step sizes for the parameter minimization.\n",
    "3. We configure the limits, if necessary.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Minuit and set up fit:\n",
    "parin   = np.array([theta, mu, sigma, xi]) # initial values (here = true values)\n",
    "parname = ['theta', 'mu', 'sigma', 'xi']\n",
    "parstep = np.array([1000., 1000., 1000., 1000.])      # initial setp sizes \n",
    "parfix  = [False, False, False, False]       # change these to fix/free parameters\n",
    "parlim  = [(None, None), (None, None), (None, None), (None, None)]    # set limits\n",
    "m = Minuit(negLogL, parin, name=parname)\n",
    "m.errors = parstep\n",
    "m.fixed = parfix\n",
    "m.limits = parlim\n",
    "m.errordef = 0.1                          # errors from lnL = lnLmax - x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618233c7-cdf3-46b9-8278-7c8e3218a492",
   "metadata": {},
   "source": [
    "Now let's dot he fit and get the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a03f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the fit, get errors, extract results\n",
    "m.migrad()                                        # minimize -logL\n",
    "MLE = m.values                                    # max-likelihood estimates\n",
    "sigmaMLE = m.errors                               # standard deviations\n",
    "cov = m.covariance                                # covariance matrix\n",
    "rho = m.covariance.correlation()                  # correlation coeffs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d170e-9a58-43e7-811f-a87bc87a7aea",
   "metadata": {},
   "source": [
    "Check the parameters values and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51240ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"par index, name, estimate, standard deviation:\")\n",
    "for i in range(m.npar):\n",
    "    if not m.fixed[i]:\n",
    "        print(\"{:4d}\".format(i), \"{:<10s}\".format(m.parameters[i]), \" = \",\n",
    "         \"{:.6f}\".format(MLE[i]), \" +/- \", \"{:.6f}\".format(sigmaMLE[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cce8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(r\"free par indices, covariance, correlation coeff.:\")\n",
    "for i in range(m.npar):\n",
    "    if not(m.fixed[i]):\n",
    "        for j in range(m.npar):\n",
    "            if not(m.fixed[j]):\n",
    "                print(i, j, \"{:.6f}\".format(cov[i,j]), \"{:.6f}\".format(rho[i,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e071c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fitted pdf\n",
    "yMin = 0.\n",
    "yMax = f(0., MLE)*1.1\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "xCurve = np.linspace(xMin, xMax, 100)\n",
    "yCurve = f(xCurve, MLE)\n",
    "plt.plot(xCurve, yCurve, color='dodgerblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff30947a-6772-409d-a5be-9950607623ab",
   "metadata": {},
   "source": [
    "# Plot data as tick marks\n",
    "tick_height = 0.05*(yMax - yMin)\n",
    "xvals = [xData, xData]\n",
    "yvals = [np.zeros_like(xData), tick_height * np.ones_like(xData)]\n",
    "plt.plot(xvals, yvals, color='black', linewidth=1)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$f(x; \\theta)$')\n",
    "plt.figtext(0.6, 0.8, r'$\\hat{\\theta} = $' + f'{MLE[0]:.4f}' +\n",
    "            r'$\\pm$' + f'{sigmaMLE[0]:.4f}')\n",
    "plt.figtext(0.6, 0.72, r'$\\hat{\\xi} = $' + f'{MLE[3]:.4f}' +\n",
    "            r'$\\pm$' + f'{sigmaMLE[3]:.4f}')\n",
    "plt.xlim(xMin, xMax)\n",
    "plt.ylim(yMin, yMax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scan of lnL (for theta, if free)\n",
    "if not(m.fixed['theta']):\n",
    "    plt.figure()\n",
    "    m.draw_mnprofile('theta')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b0c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a contour plot of lnL = lnLmax - 1/2 (here for theta and xi).\n",
    "# The tangents to this contour give the standard deviations.\n",
    "CL = stats.chi2.cdf(1.,2)            #  Q_alpha = 1, npar = 2\n",
    "print('CL = ', CL)\n",
    "if not(m.fixed['theta'] | m.fixed['xi']):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    con = m.mncontour('theta', 'xi', cl=CL, size=200)\n",
    "    con = np.vstack([con, con[0]])         # close contour\n",
    "    plt.plot(MLE[0], MLE[3], marker='o', linestyle='None', color='black', label=r'$(\\hat{\\theta}, \\hat{\\xi})$')\n",
    "    plt.plot(con[:,0], con[:,1], color='black', linewidth=1)\n",
    "    plt.xlabel(r'$\\theta$', labelpad=5)\n",
    "    plt.ylabel(r'$\\xi$', labelpad=5)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.legend(handles, labels, loc='upper right', fontsize=14, frameon=False)\n",
    "    plt.figtext(0.4, 0.93, r'$\\ln L = \\ln L_{\\rm max} - 1/2$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace86483-a46e-43de-a2c9-5242192c1b9e",
   "metadata": {},
   "source": [
    "# Confidence region from lnL = lnLmax - Q/2 (here for theta and xi)\n",
    "# where Q is the chi2 quantile of CL = 1-alpha = 0.683 and 0.95 for 2 dof.\n",
    "if not(m.fixed['theta'] | m.fixed['xi']):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    m.draw_mncontour('theta', 'xi', cl=[0.683, 0.95], size=200);\n",
    "    plt.plot(MLE[0], MLE[3], marker='o', linestyle='None', color='black', label=r'$(\\hat{\\theta}, \\hat{\\xi})$')\n",
    "    plt.xlabel(r'$\\theta$', labelpad=10)\n",
    "    plt.ylabel(r'$\\xi$', labelpad=10)\n",
    "    plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.2)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    plt.legend(handles, labels, loc='upper right', fontsize=14, frameon=False)\n",
    "    plt.figtext(0.3, 0.93, r'$\\ln L = \\ln L_{\\rm max} - \\frac{1}{2} F^{-1}_{\\chi^2}(1-\\alpha;n)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ee405-536c-4753-9a4a-a47509e128ea",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "\n",
    "Now checkout `datasets/dataset[1-5].csv`. Repeat the fit for all of them and plot the change of the $\\sigma(\\theta)$ versus sample size. Which relation do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9eec9-4c33-4be4-bbb2-af48cfb69c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e73eb2-7937-4dab-9642-c19f18fa2a33",
   "metadata": {},
   "source": [
    "Often we have some idea of our parameter of interest phases space even before we fit our data. For example, we might know that one of our nuisance parameters has been measured before with a certain uncertainty and we would like to use this prior knowledge to constrain it.   \n",
    "\n",
    "For instance, let's assume we have a generic likelihood $\\mathcal{L}$, that depends on the nuisance parameter $a$:\n",
    "$$\\mathcal{L} = \\prod_{i}^{N} P(x_{i}| a)$$\n",
    "However, the nuisance parameter $a$ is known to be constrained with PDF $R(a| a', \\sigma_a)$. This modifies the likelihood and adds a constraint term to the expression: \n",
    "$$\\mathcal{L} = \\prod_{i}^{N} P(x_{i}| a) R(a| a', \\sigma_a)$$.\n",
    "Now this also modifies the NNL that we want to minimize. \n",
    "\n",
    "# Task 3\n",
    "\n",
    "Now let's assume that instead of fixing the mass $\\mu$ we actually include the information from the previous measurement which established $\\mu$ to be $10.0 \\pm 0.2$. Implement the Gaussian constraint to the negative likelihood function reflecting this prior. \n",
    "\n",
    "Note: for the Gaussian constraint $\\mu \\sim G(\\mu', \\sigma_\\mu)$, the NLL changes as : \n",
    "$$\\text{NLL}_{constrained} = \\text{NLL}_{unconstrained} + \\frac{(\\mu - \\mu')^2}{2\\sigma_\\mu^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e994cd8-05e1-407b-8ad5-d80909f2b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here comes your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43611e69-514c-4da8-bbbb-0b75a91c88f1",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Commonly in flavour physics we use weights to simplify the final negLogL fit. Sometimes, the weights might be performing the statistical background subtraction, likesPlot weights or COWs. Sometimes, the weights can account for the detector and reconstruction effects, like efficiency weights. That way or another, the effect of weighting has to be accounted for in the likelihood. \n",
    "\n",
    "The likelihood where each data point is weighted by weight $w_{i}$ becomes : \n",
    "$$L_{weighted} = \\prod_{i}^{N} P(x_{i}| \\theta)^{w_{i}}$$\n",
    "If one applies the standard definition of the $\\theta$ uncertainties to such weighted likelihood : \n",
    "$$V_{\\theta} = - \\Big[\\frac{\\partial^2 L }{\\partial \\theta^2}\\Big]\\Big|_{\\theta = \\hat{theta}}$$\n",
    "the resulting uncertainty is *underestimated*.\n",
    "\n",
    "This effect can be corrected in multiple ways. \n",
    "\n",
    "1. Introducing a scaling factor $\\alpha$ to the likelihood [0905.0724](https://arxiv.org/abs/0905.0724):  \n",
    "$$\\alpha = \\frac{\\sum^{N}_{i} w_{i}}{\\sum_{i}^N w_i^2}$$, \n",
    "such that the NLL corresponds to the $$NLL = -\\alpha \\sum_i w_i ln P_{i} $$. \n",
    "\n",
    "The RooFit users might recognize this as the the fit configuration options called `SumW2Errors(kFalse)`.\n",
    "\n",
    "This will rescale the weights so that their sum corresponds to the effective sample size. \n",
    "\n",
    "2. Using square weighted Hessian matrix: \n",
    "\n",
    "$$V_{ij} = H_{ik}^{-1} W_{kl} H_{lj}^{-1}$$, \n",
    "\n",
    "where $H$ is a weighted Hessian matrix and $W$ is a squared weighted Hessian matrix defiend as : \n",
    "$$W_{kl} = - \\sum_{i}^N w_i^2 \\frac{\\partial^2 L(x_i | \\vec{\\theta}) }{\\partial \\theta_i \\partial \\theta_j}$$\n",
    "\n",
    "The RooFit users might recognize this as the the fit configuration options called `SumW2Errors(kTrue)`.\n",
    "\n",
    "3. A common method for estimating parameter uncertainties is bootstrapping. \n",
    "This involves repeatedly resampling the original dataset with replacement to generate new samples. \n",
    "Each resampled dataset is then used to refit the parameters. \n",
    "The standard deviation of these estimated values across all samples serves as an estimate of the parameter uncertainty. \n",
    "While this technique is broadly applicable and statistically sound, it typically requires solving numerically thousands of times, which can be computationally intensive. \n",
    "\n",
    "4. Asymptotic Errors\n",
    "\n",
    "The weighted Hessian matrix method is in general not asymptotically correct for cases where \n",
    "\n",
    "$$E\\Big(\\sum_{e=1}^{N} w_e^2 \\frac{\\partial \\ln L(x_e; \\theta)}{\\partial \\theta_k} \\frac{\\partial \\ln L(x_e; \\theta)}{\\partial \\theta_l} |_{\\hat{\\lambda}} \\Big) \\neq -E \\Big( \\sum_{e=1}^{N} w_e^2  \\frac{\\partial^2 \\ln L(x_e; \\theta)}{\\partial \\theta_k \\partial \\theta_l} |_{\\hat{\\theta}} \\Big)$$\n",
    "\n",
    "The combat these cases, the AsymptoticErrors method is used [1911.01303](https://arxiv.org/pdf/1911.01303):\n",
    "\n",
    "$$C_{ij} = \\sum_{k,l=1}^{N_P} H^{-1}_{ik} D_{kl} H^{-1}_{lj} \\Big|_{\\hat{\\theta}}\n",
    " $$\n",
    "\n",
    "where $H^{-1}_{ik}$ is weighted Hessian and \n",
    "$$D_{kl} = \\sum_{e=1}^{N} w_e^2 \n",
    "\\frac{\\partial \\ln P(x_e; \\theta)}{\\partial \\theta_k}\n",
    "\\frac{\\partial \\ln P(x_e; \\theta)}{\\partial \\theta_l}$$. \n",
    "\n",
    "Asymptotic methos is currently the recommended default approach. \n",
    "It typically gives results very similar to those obtained by the bootstrapping. \n",
    "\n",
    "In the following example you will generate a decay time distribution with known true lifetime. However, it will be modified by the efficiency, modelled as 1-sigmoid. \n",
    "\n",
    "First, figure out what is the bias from the detector inefficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe02b40-1abe-429d-b1b9-2af25f480ee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expit  \n\u001b[0;32m----> 3\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#generate sample\u001b[39;00m\n\u001b[1;32m      6\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500000\u001b[39m                 \n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit  \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#generate sample\n",
    "n_samples = 500000                 \n",
    "tau = 1.5                       \n",
    "x = np.random.exponential(tau, n_samples) \n",
    "\n",
    "def efficiency(x, x0=7.0, k=0.7):\n",
    "    return 1.0 - expit((x - x0) / k)\n",
    "\n",
    "x_vals = np.linspace(0, 10, 1000)\n",
    "eff_curve = efficiency(x_vals)\n",
    "\n",
    "#apply eff\n",
    "eff_values = efficiency(x)\n",
    "accepted_mask = np.random.rand(n_samples) < eff_values\n",
    "x_observed = x[accepted_mask]\n",
    "\n",
    "bins = np.linspace(0, 10, 100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(x, bins=bins, histtype='step', label='True Exponential (no efficiency)', density=True)\n",
    "plt.hist(x_observed, bins=bins, histtype='stepfilled', alpha=0.5, label='Observed (after efficiency)', density=True)\n",
    "x_vals = np.linspace(0, 10, 500)\n",
    "plt.xlabel(\"t [ps]\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8e26f-c469-4c12-bb61-73f7be96a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neglog_likelihood(tau):\n",
    "    if tau <= 0:\n",
    "        return 1e10 \n",
    "    norm = 1 / tau * np.exp(-x_observed / tau)\n",
    "    log_likelihood = np.log(norm)\n",
    "    return -np.sum(log_likelihood)\n",
    "# Here comes your code\n",
    "\n",
    "#m = Minuit(neglog_likelihood, tau=1.0)\n",
    "#m.errordef = 0.5\n",
    "#m.migrad()\n",
    "\n",
    "\n",
    "x_vals = np.linspace(0, 10, 300)\n",
    "pdf_true = (1 / tau_true) * np.exp(-x_vals / tau_true)\n",
    "#pdf_fit = (1 / m.values['tau']) * np.exp(-x_vals / m.values['tau'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(x_observed, bins=60, range=(0, 10), density=True, alpha=0.2, label='Observed (biased)', color='blue')\n",
    "plt.plot(x_vals, pdf_true, 'g--', label=f'True PDF (τ = {tau_true})')\n",
    "plt.plot(x_vals, pdf_fit, 'b-', label=f'Fitted PDF (τ = {m.values[\"tau\"]:.3f}+/- {m.errors[\"tau\"]:.3f})')\n",
    "plt.xlabel(\"t [ps]\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f1f86-fae3-4b9c-a224-8d99ed0bc1b7",
   "metadata": {},
   "source": [
    "Now that you have observed the efficiency bias, it becomes obvious why one must accoutn for efficiency. In the next step derive the efficiency weights per event and check if the bias has dissapeared after minimixing the weighted likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc327e25-2b30-4ab4-a4d7-524538c83f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neglog_likelihood_weighted(tau):\n",
    "    if tau <= 0:\n",
    "        return 1e10\n",
    "    log_pdf = np.log((1 / tau) * np.exp(-x_observed / tau))\n",
    "    return -np.sum(weights * log_pdf)\n",
    "\n",
    "#Here define weights\n",
    "\n",
    "weights = np.array()\n",
    "\n",
    "#mw = Minuit(neglog_likelihood_weighted, tau=1.0)\n",
    "#mw.errordef = 0.5  \n",
    "#mw.migrad()\n",
    "pdf_wfit = (1 / mw.values['tau']) * np.exp(-x_vals / mw.values['tau'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(x_observed, bins=60, range=(0, 10), density=True, alpha=0.2, label='Observed (biased)', color='blue')\n",
    "plt.hist(x_observed, bins=60, weights = weights, range=(0, 10), density=True, alpha=0.5, label='Observed (biased)', color='red')\n",
    "plt.plot(x_vals, pdf_true, 'g--', label=f'True PDF (τ = {tau_true})')\n",
    "plt.plot(x_vals, pdf_fit, 'b-', label=f'Fitted PDF (τ = {m.values[\"tau\"]:.3f}+/- {m.errors[\"tau\"]:.3f})')\n",
    "plt.plot(x_vals, pdf_wfit, 'r-', label=f'Fitted weighted PDF (τ = {mw.values[\"tau\"]:.3f} +/- {mw.errors[\"tau\"]:.3f})')\n",
    "plt.xlabel(\"t [ps]\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1d06b-24de-4d28-878b-f3c0a408633a",
   "metadata": {},
   "source": [
    "Now compare different uncertainty estimations for the weighted likelihood fit with the baseline result you obtained in the previous step. Which uncertainty do you trust the most and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f1ed30-fd3b-4262-a3c7-9601d769d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_with_alpha(x, w):\n",
    "    sumw = np.sum(w)\n",
    "    sumw2 = np.sum(w**2)\n",
    "    alpha = sumw / sumw2\n",
    "\n",
    "    def neglog_likelihood_scaled(tau):\n",
    "        if tau <= 0:\n",
    "            return 1e10\n",
    "        log_pdf = np.log((1 / tau) * np.exp(-x / tau))\n",
    "        return -alpha * np.sum(w * log_pdf)\n",
    "      \n",
    "\n",
    "    ms = Minuit(neglog_likelihood_scaled, tau=1.0)\n",
    "    ms.errordef = 0.5\n",
    "    ms.migrad()\n",
    "    ms.hesse()\n",
    "    return ms.values['tau'], ms.errors['tau']\n",
    "\n",
    "#res, err = fit_with_alpha(x_observed, weights)\n",
    "#print(f\"tau = {res:.4f} +/- {err:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c8e13-35af-4db5-a018-cb49741604a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted hessian\n",
    "\n",
    "def fit_weighted_hessian(x, w):\n",
    "\n",
    "    def second_derivatives(x, tau):\n",
    "        return (1 / tau**2) - (2 * x / tau**3)\n",
    "\n",
    "    def neglog_likelihood_weighted(tau):\n",
    "        if tau <= 0:\n",
    "            return 1e10\n",
    "        log_pdf = np.log((1 / tau) * np.exp(-x / tau))\n",
    "        return -np.sum(w * log_pdf)\n",
    "\n",
    "    mhw = Minuit(neglog_likelihood_weighted, tau=1.0)\n",
    "    mhw.errordef = 0.5\n",
    "    mhw.migrad()\n",
    "    mhw.hesse()\n",
    "\n",
    "    H = np.sum(w * second_derivatives(x, mhw.values['tau']))\n",
    "    W = -1*np.sum(w**2 * second_derivatives(x, mhw.values['tau']))\n",
    "    V = (1 / H)**2 * W\n",
    "\n",
    "    error_corrected = np.sqrt(V)\n",
    "    return mhw.values['tau'], error_corrected\n",
    "\n",
    "#res, err = fit_weighted_hessian(x_observed, weights)\n",
    "#print(f\"tau = {res:.4f} +/- {err:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf700cff-9970-4c73-9b6e-52889c3b76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bootstrap(x, w, n_bootstrap = 100):\n",
    "\n",
    "    \n",
    "    x_fit = x\n",
    "    w_fit = w\n",
    "    def neglog_likelihood_weighted(tau):\n",
    "        if tau <= 0:\n",
    "            return 1e10\n",
    "        log_pdf = np.log((1 / tau) * np.exp(-x_fit / tau))\n",
    "        return -np.sum(w_fit * log_pdf)\n",
    "\n",
    "    bootstrap_taus = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_array = np.zeros(len(x))\n",
    "        bootstrap_weight = np.zeros(len(w))\n",
    "        indices = np.random.randint(0, len(x), size=len(x))\n",
    "        x_fit = x[indices]\n",
    "        w_fit = w[indices]\n",
    "\n",
    "\n",
    "        m_bs = Minuit(neglog_likelihood_weighted, tau=1.0)\n",
    "        m_bs.errordef = 0.5\n",
    "        m_bs.migrad()\n",
    "\n",
    "        if m_bs.valid:\n",
    "            bootstrap_taus.append(m_bs.values[\"tau\"])\n",
    "        \n",
    "        \n",
    "    tau_bootstrap_mean = np.mean(bootstrap_taus)\n",
    "    tau_bootstrap_std = np.std(bootstrap_taus)\n",
    "    \n",
    "    return tau_bootstrap_mean, tau_bootstrap_std\n",
    "\n",
    "#res, err = fit_bootstrap(x_observed, weights)\n",
    "#print(f\"tau = {res:.4f} +/- {err:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29faa65f-6c66-40f8-bf39-5dcf5f38c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_asymptotic(x, w):\n",
    "\n",
    "    def neglog_likelihood_weighted(tau):\n",
    "        if tau <= 0:\n",
    "            return 1e10\n",
    "        log_pdf = np.log((1 / tau) * np.exp(-x / tau))\n",
    "        return -np.sum(w * log_pdf)\n",
    "\n",
    "    mas = Minuit(neglog_likelihood_weighted, tau=1.0)\n",
    "    mas.errordef = 0.5\n",
    "    mas.migrad()\n",
    "    mas.hesse()\n",
    "    def first_derivatives(x, tau):\n",
    "        return -1 / tau + x / tau**2\n",
    "    \n",
    "    def second_derivatives(x, tau):\n",
    "        return (1 / tau**2) - (2 * x / tau**3)\n",
    "\n",
    "    D = np.sum(w**2 * first_derivatives(x, mas.values['tau'])**2 )\n",
    "    H = np.sum(w * second_derivatives(x, mas.values['tau']))\n",
    "    V = (1 / H)**2 * D\n",
    "    error_corrected = np.sqrt(V)   \n",
    "    return mas.values['tau'], error_corrected\n",
    "\n",
    "#res, err = fit_asymptotic(x_observed, weights)\n",
    "#print(f\"tau = {res:.4f} +/- {err:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9cc71b-d032-47d1-a01f-2da1a0a4bdc6",
   "metadata": {},
   "source": [
    "In the last step throw multiple toys and compare the pulls between different types of uncertainty computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad023c-b7ac-406d-9981-b0d63e5b25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull(est, gen, err):\n",
    "    return (est - gen) / err if err > 0 else 0.0\n",
    "\n",
    "n_toys = 300\n",
    "n_stats = 1000\n",
    "tau_gen = 1.5\n",
    "pulls_alpha, pulls_weighted_hessian, pulls_bootstrap, pulls_asymptotic = [], [], [], []\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for _ in tqdm(range(n_toys), desc=\"Toys\"):\n",
    "    x_true = np.random.exponential(tau_gen, n_stats)\n",
    "    eff = efficiency(x_true)\n",
    "    accept = np.random.rand(n_stats) < eff\n",
    "    x_fit = x_true[accept]\n",
    "    w_fit = 1.0 / np.clip(efficiency(x_fit), 1e-3, 1.0) \n",
    "\n",
    "    tau_a, err_a = fit_asymptotic(x_fit, w_fit)\n",
    "    tau_d, err_d = fit_with_alpha(x_fit, w_fit)\n",
    "    tau_s, err_s = fit_weighted_hessian(x_fit, w_fit)\n",
    "    tau_b, err_b = fit_bootstrap(x_fit, w_fit)\n",
    "\n",
    "    pulls_alpha.append(pull(tau_d, tau_gen, err_d))\n",
    "    pulls_weighted_hessian.append(pull(tau_s, tau_gen, err_s))\n",
    "    pulls_bootstrap.append(pull(tau_b, tau_gen, err_b))\n",
    "    pulls_asymptotic.append(pull(tau_a, tau_gen, err_a))\n",
    "    \n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(18, 5), sharey=True)\n",
    "methods = ['Alpha', 'WeightedHessian', 'Bootstrap', 'Asymptotic']\n",
    "pull_data = [pulls_alpha, pulls_weighted_hessian, pulls_bootstrap, pulls_asymptotic]\n",
    "\n",
    "from scipy.stats import norm \n",
    "\n",
    "for i, (ax, pulls, label) in enumerate(zip(axs, pull_data, methods)):\n",
    "    ax.hist(pulls, bins=30, density=True, alpha=0.7)\n",
    "    mu, std = norm.fit(pulls)\n",
    "    x_vals = np.linspace(-2, 2, 500)\n",
    "    ax.plot(x_vals, norm.pdf(x_vals, mu, std), 'r--', label=f'mu={mu:.2f}, sigma={std:.2f}')\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.set_xlabel('Pull')\n",
    "    ax.axvline(0, color='black', linestyle=':')\n",
    "    ax.legend()\n",
    "\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf176c0-b981-431e-a610-c9cda88f95a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/zfit/__init__.py:60: UserWarning: TensorFlow warnings are by default suppressed by zfit. In order to show them, set the environment variable ZFIT_DISABLE_TF_WARNINGS=0. In order to suppress the TensorFlow warnings AND this warning, set ZFIT_DISABLE_TF_WARNINGS=1.\n",
      "  warnings.warn(\n",
      "2025-05-08 20:56:34.387876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746737794.401367     579 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746737794.405356     579 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746737794.417354     579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746737794.417366     579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746737794.417368     579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746737794.417369     579 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "#Here is your solution\n",
    "import ROOT\n",
    "import zfit"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
