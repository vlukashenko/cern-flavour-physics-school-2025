{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"42f5fbf3","cell_type":"markdown","source":"Example of maximum-likelihood fit with iminuit version 2.\npdf is a mixture of Gaussian (signal) and exponential (background),\ntruncated in [xMin,xMax].\nG. Cowan / RHUL Physics / December 2021","metadata":{}},{"id":"262ac5fe-e891-4cfc-9d4b-51989182cab5","cell_type":"markdown","source":"We first will start with importing all the necessary packages. The exercise is based on python implementation of Minuit package. We will use it to perform an unbinned MLE fit to the generated mass-spectrum with one peak. ","metadata":{}},{"id":"dbac3347","cell_type":"code","source":"import numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import truncexpon #truncated exponential in [a,b]\nfrom scipy.stats import truncnorm  #truncated normal distribution in [a,b]\nfrom scipy.stats import chi2\nimport iminuit\nfrom iminuit import Minuit\nimport matplotlib.pyplot as plt\nfrom matplotlib import container\nplt.rcParams[\"font.size\"] = 14\nprint(\"iminuit version:\", iminuit.__version__)  # need 2.x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ba7a931e","cell_type":"code","source":"import mplhep as hep\nhep.style.use(\"ROOT\") # Or CMS, ATLAS, LHCb2...","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"245a49a7-2891-4861-8cad-15922b0359ff","cell_type":"markdown","source":"Now, here we will upload the dataset generated for you beforehand. ","metadata":{}},{"id":"c0e46b84","cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"../datasets/dataset.csv\", header=None)\nxData = df.values  ","metadata":{"lines_to_next_cell":1,"trusted":true},"outputs":[],"execution_count":null},{"id":"08859d87","cell_type":"markdown","source":"# Task 0\n\nGet familiar with the dataset. Can you already guess which models you will need to use to fit it? How many parameters you are going to estimate? Write a python function that takes as input on data `x` and parameters `par` and returns the pdf. ","metadata":{}},{"id":"7c8a7e2e","cell_type":"code","source":"df.plot.hist(bins=100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c018d0f2-5c86-4532-aa95-5cbc62241b5b","cell_type":"code","source":"#Put your solution here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"03b849d5-d000-4941-9f9f-7678838d4363","cell_type":"markdown","source":"<details>\n<summary><strong>Click to show the solution</strong></summary>\n\nSolution: The model we will use is rather simple. The signal is modelled with a single Gaussian, while the background is modelled with exponential. The total PDF is defined as following :\n$$f(m) = \\theta \\cdot G(m; \\mu, \\sigma) + (1 - \\theta)\\cdot Exp(m; \\xi)$$\nwhere $\\theta$ is signal fraction and $\\xi$ is exponential power. \n\n```python\ndef f(x, par):\n    theta   = par[0]\n    mu      = par[1]\n    sigma   = par[2]\n    xi      = par[3]\n    fs = stats.truncnorm.pdf(x, a=(xMin-mu)/sigma, b=(xMax-mu)/sigma, loc=mu, scale=sigma)\n    fb = stats.truncexpon.pdf(x, b=(xMax-xMin)/xi, loc=xMin, scale=xi)\n    return theta*fs + (1-theta)*fb\n```\n</details>  \n","metadata":{"lines_to_next_cell":1}},{"id":"7ff81451-9269-408a-b499-8ebfe400ab28","cell_type":"markdown","source":"We will now implement the nLL ourselves. Note, that Minuit expects the nLL python function to depend ONLY on the parameters. ","metadata":{"lines_to_next_cell":1}},{"id":"0d3c2793-9eb6-4b9a-b255-249575805bb6","cell_type":"code","source":"def negLogL(par):\n    pdf = f(xData, par)\n    return -np.sum(np.log(pdf))","metadata":{"lines_to_next_cell":1,"trusted":true},"outputs":[],"execution_count":null},{"id":"950cd3d5-035e-4849-98b8-7464f6416082","cell_type":"markdown","source":"# Task 1 \n\nConfigure the iminuit fit. For this we need to setup the 2 POI $\\theta$ and $\\xi$ and 2 nuisance parameters $\\mu$ and $\\sigma$. \n1. We configure initial values\n2. We configure initial step sizes for the parameter minimization.\n3. We configure the limits, if necessary.\n\n   ","metadata":{}},{"id":"8deb458c","cell_type":"code","source":"# Initialize Minuit and set up fit:\nparin   = np.array([1, 1, 1, 1]) \nparname = ['theta', 'mu', 'sigma', 'xi'] #initial values\nparstep = np.array([1000., 1000., 1000., 1000.])      # initial setp sizes \nparfix  = [False, False, False, False]       # change these to fix/free parameters\nparlim  = [(None, None), (None, None), (None, None), (None, None)]    # set limits\n  \n# This was missing\nxMin=0.\nxMax=20.\n\nm = Minuit(negLogL, parin, name=parname)\nm.errors = parstep\nm.fixed = parfix\nm.limits = parlim\nm.errordef = 0.5                          # errors from lnL = lnLmax - x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cb5a03f8","cell_type":"code","source":"# Do the fit, get errors, extract results\nm.migrad()                                        # minimize -logL\nMLE = m.values                                    # max-likelihood estimates\nsigmaMLE = m.errors                               # standard deviations\ncov = m.covariance                                # covariance matrix\nrho = m.covariance.correlation()                  # correlation coeffs.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1fa40544-13fc-406f-965c-dc0776310af9","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"28bbfc0f-23bf-4ce1-934b-a07fce0e1961","cell_type":"code","source":"#Put your solution here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cb0d0a51-ce16-47e7-ace1-25e849d03a10","cell_type":"markdown","source":"<details>\n<summary><strong>Click to show the solution</strong></summary>\n\n```python\nparin   = np.array([0.2, 10., 2., 5.])      # initial values \nparname = ['theta', 'mu', 'sigma', 'xi']\nparname_latex = [r'$\\theta$', r'$\\mu$', r'$\\sigma$', r'$\\xi$']\nparstep = np.array([0.1, 1., 1., 1.])           # initial setp sizes\nparfix  = [False, True, True, False]            # change to fix/free param.\nparlim  = [(0.,1), (None, None), (0., None), (0., None)]\n```\n</details> ","metadata":{}},{"id":"618233c7-cdf3-46b9-8278-7c8e3218a492","cell_type":"markdown","source":"Now let's do the fit and get the covariance.","metadata":{}},{"id":"848d170e-9a58-43e7-811f-a87bc87a7aea","cell_type":"markdown","source":"Check the parameters values and standard deviation. ","metadata":{}},{"id":"e51240ff","cell_type":"code","source":"print(r\"par index, name, estimate, standard deviation:\")\nfor i in range(m.npar):\n    if not m.fixed[i]:\n        print(\"{:4d}\".format(i), \"{:<10s}\".format(m.parameters[i]), \" = \",\n         \"{:.6f}\".format(MLE[i]), \" +/- \", \"{:.6f}\".format(sigmaMLE[i]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"77cce8a9","cell_type":"code","source":"print(r\"free par indices, covariance, correlation coeff.:\")\nfor i in range(m.npar):\n    if not(m.fixed[i]):\n        for j in range(m.npar):\n            if not(m.fixed[j]):\n                print(i, j, \"{:.6f}\".format(cov[i,j]), \"{:.6f}\".format(rho[i,j]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8487af95-0482-43fb-a63c-5b5302d5fb67","cell_type":"markdown","source":"Let's plot fitted data ","metadata":{}},{"id":"e071c19f","cell_type":"code","source":"# Plot fitted pdf\nyMin = 0.\nyMax = f(0., MLE)*1.1\nfig = plt.figure(figsize=(8,6))\nxCurve = np.linspace(xMin, xMax, 100)\nyCurve = f(xCurve, MLE)\nplt.plot(xCurve, yCurve, color='dodgerblue')\n# Plot data as tick marks\ntick_height = 0.05*(yMax - yMin)\nx = np.asarray(xData).flatten()\nxvals = np.vstack([x, x])\nyvals = np.vstack([np.zeros_like(x), tick_height * np.ones_like(x)])\nplt.plot(xvals, yvals, color='black', linewidth=1)\nplt.xlabel(r'$x$')\nplt.ylabel(r'$f(x; \\theta)$')\nplt.figtext(0.6, 0.8, r'$\\hat{\\theta} = $' + f'{MLE[0]:.4f}' +\n            r'$\\pm$' + f'{sigmaMLE[0]:.4f}')\nplt.figtext(0.6, 0.72, r'$\\hat{\\xi} = $' + f'{MLE[3]:.4f}' +\n            r'$\\pm$' + f'{sigmaMLE[3]:.4f}')\nplt.xlim(xMin, xMax)\nplt.ylim(yMin, yMax)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"861ffe89-815d-47d4-8592-59fef8c0e35a","cell_type":"markdown","source":"Make a lnL profile plot ","metadata":{}},{"id":"5c2c0e0e","cell_type":"code","source":"# Make scan of lnL (for theta, if free)\nif not(m.fixed['theta']):\n    plt.figure()\n    m.draw_mnprofile('theta')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e46c2e27-8617-4615-b355-ee1bdcaecd83","cell_type":"markdown","source":"Make a contour plot of $\\ln L = \\ln L_{max} - 1/2$ (here for $\\theta$ and $\\xi$).\nThe tangents to this contour give the standard deviations.","metadata":{}},{"id":"46b0c1bf","cell_type":"code","source":"Q_alpha = #your solution\nndof = #your solution \n\nCL = stats.chi2.cdf(Q_alpha, ndof)            \nif not(m.fixed['theta'] | m.fixed['xi']):\n    fig, ax = plt.subplots(1,1)\n    con = m.mncontour('theta', 'xi', cl=CL, size=200)\n    con = np.vstack([con, con[0]])         # close contour\n    plt.plot(MLE[0], MLE[3], marker='o', linestyle='None', color='black', label=r'$(\\hat{\\theta}, \\hat{\\xi})$')\n    plt.plot(con[:,0], con[:,1], color='black', linewidth=1)\n    plt.xlabel(r'$\\theta$', labelpad=5)\n    plt.ylabel(r'$\\xi$', labelpad=5)\n    handles, labels = ax.get_legend_handles_labels()\n    plt.legend(handles, labels, loc='upper right', fontsize=14, frameon=False)\n    plt.figtext(0.4, 0.93, r'$\\ln L = \\ln L_{\\rm max} - 1/2$')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7aa14180-989b-486a-910e-8f599f75f70e","cell_type":"markdown","source":"Confidence region from $\\ln L = \\ln L_{max} - Q/2$ (here for $\\theta$ and $\\xi$)\nwhere Q is the chi2 quantile of CL = 1-alpha = 0.683 and 0.95 for 2 dof.","metadata":{}},{"id":"52f35440-36e7-40dd-b1a2-5ede2a5f0519","cell_type":"code","source":"if not(m.fixed['theta'] | m.fixed['xi']):\n    fig, ax = plt.subplots(1,1)\n    m.draw_mncontour('theta', 'xi', cl=[0.683, 0.95], size=200);\n    plt.plot(MLE[0], MLE[3], marker='o', linestyle='None', color='black', label=r'$(\\hat{\\theta}, \\hat{\\xi})$')\n    plt.xlabel(r'$\\theta$', labelpad=10)\n    plt.ylabel(r'$\\xi$', labelpad=10)\n    plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.2)\n    handles, labels = ax.get_legend_handles_labels()\n    plt.legend(handles, labels, loc='upper right', fontsize=14, frameon=False)\n    plt.figtext(0.3, 0.93, r'$\\ln L = \\ln L_{\\rm max} - \\frac{1}{2} F^{-1}_{\\chi^2}(1-\\alpha;n)$')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fc8ee405-536c-4753-9a4a-a47509e128ea","cell_type":"markdown","source":"# Task 2\n\n\nNow checkout `datasets/dataset[1-5].csv`. Repeat the fit for all of them and plot the change of the $\\sigma(\\theta)$ versus sample size. Which relation do you observe?","metadata":{}},{"id":"e4b9eec9-4c33-4be4-bbb2-af48cfb69c7d","cell_type":"code","source":"#Add your code here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b1e73eb2-7937-4dab-9642-c19f18fa2a33","cell_type":"markdown","source":"\n# Task 3\n\nOften we have some idea of our parameter phases space even before we fit our data. For example, we might know that one of our nuisance parameters has been measured before with a certain uncertainty and we would like to use this prior knowledge to constrain it.   \n\nFor instance, let's assume we have a generic likelihood $\\mathcal{L}$, that depends on the nuisance parameter $a$:\n$$\\mathcal{L} = \\prod_{i}^{N} P(x_{i}| a)$$\nHowever, the nuisance parameter $a$ is known to be constrained with PDF $R(a| a', \\sigma_a)$. This modifies the likelihood and adds a constraint term to the expression: \n$$\\mathcal{L} = \\prod_{i}^{N} P(x_{i}| a) R(a| a', \\sigma_a)$$.\nNow this also modifies the NNL that we want to minimize. \n\nNow let's assume that instead of fixing the mass $\\mu$ we actually include the information from the previous measurement which established $\\mu$ to be $10.0 \\pm 0.2$. Implement the Gaussian constraint to the negative likelihood function reflecting this prior and rerun the fit. \n\nNote: for the Gaussian constraint $\\mu \\sim G(\\mu', \\sigma_\\mu)$, the NLL changes as : \n$$\\text{NLL}_{constrained} = \\text{NLL}_{unconstrained} + \\frac{(\\mu - \\mu')^2}{2\\sigma_\\mu^2}$$","metadata":{}},{"id":"6e994cd8-05e1-407b-8ad5-d80909f2b5ce","cell_type":"code","source":"#Here comes your solution\n#def negLogL","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d34b5045-f65b-4726-8b27-3ca7dacfc7ba","cell_type":"markdown","source":"<details>\n<summary><strong>Click to show the solution</strong></summary>\n    \n```python\ndef negLogL(par):  \n    mu      = par[1]\n    pdf = f(xData, par)\n    logL = np.sum(np.log(pdf))\n    constraint = 0.5 * ((mu - 10.0) / 0.2) ** 2\n\n    return -logL + constraint\n```\n</details>   \n    ","metadata":{}},{"id":"43611e69-514c-4da8-bbbb-0b75a91c88f1","cell_type":"markdown","source":"# Task 4\n\nCommonly in flavour physics we use weights to simplify the final negLogL fit. Sometimes, the weights might be performing the statistical background subtraction, likesPlot weights or COWs. Sometimes, the weights can account for the detector and reconstruction effects, like efficiency weights. That way or another, the effect of weighting has to be accounted for in the likelihood. \n\nThe likelihood where each data point is weighted by weight $w_{i}$ becomes : \n$$L_{weighted} = \\prod_{i}^{N} P(x_{i}| \\theta)^{w_{i}}$$\nIf one applies the standard definition of the $\\theta$ uncertainties to such weighted likelihood : \n$$V_{\\theta} = - \\Big[\\frac{\\partial^2 L }{\\partial \\theta^2}\\Big]\\Big|_{\\theta = \\hat{\\theta}}$$\nthe resulting uncertainty is *underestimated*.\n\nThis effect can be corrected in multiple ways. \n\n1. Introducing a scaling factor $\\alpha$ to the likelihood [0905.0724](https://arxiv.org/abs/0905.0724):  \n$\\alpha = \\frac{\\sum^{N}_{i} w_{i}}{\\sum_{i}^N w_i^2}$, such that the NLL corresponds to the $NLL = -\\alpha \\sum_i w_i ln P_{i} $. \n\nThe RooFit users might recognize this as the the fit configuration options called `SumW2Errors(kFalse)`.\n\nThis will rescale the weights so that their sum corresponds to the effective sample size. \n\n2. Using square weighted Hessian matrix: \n\n$$V_{ij} = H_{ik}^{-1} W_{kl} H_{lj}^{-1}$$, \n\nwhere $H$ is a weighted Hessian matrix and $W$ is a squared weighted Hessian matrix defiend as : \n$$W_{kl} = - \\sum_{i}^N w_i^2 \\frac{\\partial^2 L(x_i | \\vec{\\theta}) }{\\partial \\theta_i \\partial \\theta_j}$$\n\nThe RooFit users might recognize this as the fit configuration options called `SumW2Errors(kTrue)`.\n\n3. A common method for estimating parameter uncertainties is bootstrapping. \nThis involves repeatedly resampling the original dataset with replacement to generate new samples. \nEach resampled dataset is then used to refit the parameters. \nThe standard deviation of these estimated values across all samples serves as an estimate of the parameter uncertainty. \nWhile this technique is broadly applicable and statistically sound, it typically requires solving numerically thousands of times, which can be computationally intensive. \n\n4. Asymptotic Errors\n\nThe weighted Hessian matrix method is in general not asymptotically correct for cases where \n\n$$E\\Big(\\sum_{e=1}^{N} w_e^2 \\frac{\\partial \\ln L(x_e; \\theta)}{\\partial \\theta_k} \\frac{\\partial \\ln L(x_e; \\theta)}{\\partial \\theta_l} |_{\\hat{\\lambda}} \\Big) \\neq -E \\Big( \\sum_{e=1}^{N} w_e^2  \\frac{\\partial^2 \\ln L(x_e; \\theta)}{\\partial \\theta_k \\partial \\theta_l} |_{\\hat{\\theta}} \\Big)$$\n\nThe combat these cases, the AsymptoticErrors method is used [1911.01303](https://arxiv.org/pdf/1911.01303):\n\n$$C_{ij} = \\sum_{k,l=1}^{N_P} H^{-1}_{ik} D_{kl} H^{-1}_{lj} \\Big|_{\\hat{\\theta}}\n $$\n\nwhere $H^{-1}_{ik}$ is weighted Hessian and \n$$D_{kl} = \\sum_{e=1}^{N} w_e^2 \n\\frac{\\partial \\ln P(x_e; \\theta)}{\\partial \\theta_k}\n\\frac{\\partial \\ln P(x_e; \\theta)}{\\partial \\theta_l}$$. \n\nAsymptotic method is currently the recommended default approach. \nIt typically gives results very similar to those obtained by the bootstrapping. \n\nIn the following example you will generate a decay time distribution with known true lifetime. However, it will be modified by the efficiency, modelled as 1-sigmoid. \n\nFirst, figure out what is the effect of the inefficeincy on the decay time distribution? How would you estimate its size? \n\n","metadata":{}},{"id":"cfe02b40-1abe-429d-b1b9-2af25f480ee0","cell_type":"code","source":"from scipy.special import expit  \n\nnp.random.seed(42)\n\n#generate sample\nn_samples = 500000                 \ntau_true = 1.5                       \nx = np.random.exponential(tau_true, n_samples) \n\ndef efficiency(x, x0=7.0, k=0.7):\n    return 1.0 - expit((x - x0) / k)\n\nx_vals = np.linspace(0, 10, 1000)\neff_curve = efficiency(x_vals)\n\n#apply eff\neff_values = efficiency(x)\naccepted_mask = np.random.rand(n_samples) < eff_values\nx_observed = x[accepted_mask]\n\nbins = np.linspace(0, 10, 100)\nplt.figure(figsize=(10, 6))\nplt.hist(x, bins=bins, histtype='step', label='True Exponential (no efficiency)', density=True)\nplt.hist(x_observed, bins=bins, histtype='stepfilled', alpha=0.5, label='Observed (after efficiency)', density=True)\nx_vals = np.linspace(0, 10, 500)\nplt.xlabel(\"t [ps]\")\nplt.ylabel(\"A.U.\")\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"af9f34e8-a155-415f-9ab1-dcf6ccef209c","cell_type":"code","source":"#Here comes your definition of neglog_likelihood\n#def neglog_likelihood","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f642d4ee-0de6-4c61-829b-9f3808cc4e5f","cell_type":"markdown","source":"<details>\n<summary><strong>Click to show the solution</strong></summary>\n    \n```python\ndef neglog_likelihood(tau):\n    if tau <= 0:\n        return 1e10 \n    norm = 1 / tau * np.exp(-x_observed / tau)\n    log_likelihood = np.log(norm)\n    return -np.sum(log_likelihood)\n```\n</details>","metadata":{}},{"id":"b2f8e26f-c469-4c12-bb61-73f7be96a11d","cell_type":"code","source":"m = Minuit(neglog_likelihood, tau=1.0)\nm.errordef = 0.5\nm.migrad()\n\nx_vals = np.linspace(0, 10, 300)\npdf_true = (1 / tau_true) * np.exp(-x_vals / tau_true)\npdf_fit = (1 / m.values['tau']) * np.exp(-x_vals / m.values['tau'])\nplt.figure(figsize=(10, 6))\nplt.hist(x_observed, bins=60, range=(0, 10), density=True, alpha=0.2, label='Observed (biased)', color='blue')\nplt.plot(x_vals, pdf_true, 'g--', label=f'True PDF (τ = {tau_true})')\nplt.plot(x_vals, pdf_fit, 'b-', label=f'Fitted PDF (τ = {m.values[\"tau\"]:.3f}+/- {m.errors[\"tau\"]:.3f})')\nplt.xlabel(\"t [ps]\")\nplt.ylabel(\"A.U.\")\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"068f1f86-fae3-4b9c-a224-8d99ed0bc1b7","cell_type":"markdown","source":"Now that you have observed the efficiency bias, it becomes obvious why one must accoutn for efficiency. In the next step derive the efficiency weights per event and check if the bias has dissapeared after minimixing the weighted likelihood. ","metadata":{}},{"id":"a22cf734-d805-4405-8170-2aea1e977bcc","cell_type":"code","source":"#Here define weights\nweights = np.array()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"134549e1-6da1-4bf1-b564-a8f93ab76297","cell_type":"markdown","source":"<details>\n<summary><strong>Click to show the solution</strong></summary>\n    \n```python\nweights = 1.0 / np.clip(efficiency(x_observed), 1e-3, 1.0) \n```\n</details>","metadata":{}},{"id":"cd8d8774-c4b2-40ab-b462-2c1c38a887f3","cell_type":"code","source":"#Here define weighted likelihood\n#def neglog_likelihood_weighted","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d2b6696f-dce9-4f2c-b41c-90a109c62864","cell_type":"markdown","source":"<details>\n<summary><strong>Click to show the solution</strong></summary>\n    \n```python\ndef neglog_likelihood_weighted(tau):\n    if tau <= 0:\n        return 1e10\n    log_pdf = np.log((1 / tau) * np.exp(-x_observed / tau))\n    return -np.sum(weights * log_pdf)\n```\n</details>","metadata":{}},{"id":"bc327e25-2b30-4ab4-a4d7-524538c83f42","cell_type":"code","source":"mw = Minuit(neglog_likelihood_weighted, tau=1.0)\nmw.errordef = 0.5  \nmw.migrad()\npdf_wfit = (1 / mw.values['tau']) * np.exp(-x_vals / mw.values['tau'])\nplt.figure(figsize=(10, 6))\nplt.hist(x_observed, bins=60, range=(0, 10), density=True, alpha=0.2, label='Observed (biased)', color='blue')\nplt.hist(x_observed, bins=60, weights = weights, range=(0, 10), density=True, alpha=0.5, label='Observed (biased)', color='red')\nplt.plot(x_vals, pdf_true, 'g--', label=f'True PDF (τ = {tau_true})')\nplt.plot(x_vals, pdf_fit, 'b-', label=f'Fitted PDF (τ = {m.values[\"tau\"]:.3f}+/- {m.errors[\"tau\"]:.3f})')\nplt.plot(x_vals, pdf_wfit, 'r-', label=f'Fitted weighted PDF (τ = {mw.values[\"tau\"]:.3f} +/- {mw.errors[\"tau\"]:.3f})')\nplt.xlabel(\"t [ps]\")\nplt.ylabel(\"A.U.\")\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a7c1d06b-24de-4d28-878b-f3c0a408633a","cell_type":"markdown","source":"Now compare different uncertainty estimations for the weighted likelihood fit with the baseline result you obtained in the previous step. Which uncertainty do you trust the most and why?","metadata":{}},{"id":"42f1ed30-fd3b-4262-a3c7-9601d769d371","cell_type":"code","source":"#1: alpha scaling \ndef fit_with_alpha(x, w):\n    sumw = np.sum(w)\n    sumw2 = np.sum(w**2)\n    alpha = sumw / sumw2\n\n    def neglog_likelihood_scaled(tau):\n        if tau <= 0:\n            return 1e10\n        log_pdf = np.log((1 / tau) * np.exp(-x / tau))\n        return -alpha * np.sum(w * log_pdf)\n      \n\n    ms = Minuit(neglog_likelihood_scaled, tau=1.0)\n    ms.errordef = 0.5\n    ms.migrad()\n    ms.hesse()\n    return ms.values['tau'], ms.errors['tau']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"501c8e13-35af-4db5-a018-cb49741604a8","cell_type":"code","source":"#2: Weighted hessian\ndef fit_weighted_hessian(x, w):\n\n    def second_derivatives(x, tau):\n        return (1 / tau**2) - (2 * x / tau**3)\n\n    def neglog_likelihood_weighted(tau):\n        if tau <= 0:\n            return 1e10\n        log_pdf = np.log((1 / tau) * np.exp(-x / tau))\n        return -np.sum(w * log_pdf)\n\n    mhw = Minuit(neglog_likelihood_weighted, tau=1.0)\n    mhw.errordef = 0.5\n    mhw.migrad()\n    mhw.hesse()\n\n    H = np.sum(w * second_derivatives(x, mhw.values['tau']))\n    W = -1*np.sum(w**2 * second_derivatives(x, mhw.values['tau']))\n    V = (1 / H)**2 * W\n\n    error_corrected = np.sqrt(V)\n    return mhw.values['tau'], error_corrected","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bf700cff-9970-4c73-9b6e-52889c3b76dd","cell_type":"code","source":"#3: bootstrap\ndef fit_bootstrap(x, w, n_bootstrap = 100):    \n    x_fit = x\n    w_fit = w\n    def neglog_likelihood_weighted(tau):\n        if tau <= 0:\n            return 1e10\n        log_pdf = np.log((1 / tau) * np.exp(-x_fit / tau))\n        return -np.sum(w_fit * log_pdf)\n\n    bootstrap_taus = []\n    for _ in range(n_bootstrap):\n        bootstrap_array = np.zeros(len(x))\n        bootstrap_weight = np.zeros(len(w))\n        indices = np.random.randint(0, len(x), size=len(x))\n        x_fit = x[indices]\n        w_fit = w[indices]\n\n\n        m_bs = Minuit(neglog_likelihood_weighted, tau=1.0)\n        m_bs.errordef = 0.5\n        m_bs.migrad()\n\n        if m_bs.valid:\n            bootstrap_taus.append(m_bs.values[\"tau\"])\n        \n        \n    tau_bootstrap_mean = np.mean(bootstrap_taus)\n    tau_bootstrap_std = np.std(bootstrap_taus)\n    \n    return tau_bootstrap_mean, tau_bootstrap_std","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"29faa65f-6c66-40f8-bf39-5dcf5f38c35e","cell_type":"code","source":"#4: asymptotic\ndef fit_asymptotic(x, w):\n\n    def neglog_likelihood_weighted(tau):\n        if tau <= 0:\n            return 1e10\n        log_pdf = np.log((1 / tau) * np.exp(-x / tau))\n        return -np.sum(w * log_pdf)\n\n    mas = Minuit(neglog_likelihood_weighted, tau=1.0)\n    mas.errordef = 0.5\n    mas.migrad()\n    mas.hesse()\n    def first_derivatives(x, tau):\n        return -1 / tau + x / tau**2\n    \n    def second_derivatives(x, tau):\n        return (1 / tau**2) - (2 * x / tau**3)\n\n    D = np.sum(w**2 * first_derivatives(x, mas.values['tau'])**2 )\n    H = np.sum(w * second_derivatives(x, mas.values['tau']))\n    V = (1 / H)**2 * D\n    error_corrected = np.sqrt(V)   \n    return mas.values['tau'], error_corrected","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7d9cc71b-d032-47d1-a01f-2da1a0a4bdc6","cell_type":"markdown","source":"In the last step throw multiple toys and compare the pulls between different types of uncertainty computation. ","metadata":{}},{"id":"57ad023c-b7ac-406d-9981-b0d63e5b25a8","cell_type":"code","source":"def pull(est, gen, err):\n    return (est - gen) / err if err > 0 else 0.0\n\nn_toys = 300\nn_stats = 1000\ntau_gen = 1.5\npulls_alpha, pulls_weighted_hessian, pulls_bootstrap, pulls_asymptotic = [], [], [], []\n\nnp.random.seed(42)\n\nfrom tqdm import tqdm\nfor _ in tqdm(range(n_toys), desc=\"Toys\"):\n    x_true = np.random.exponential(tau_gen, n_stats)\n    eff = efficiency(x_true)\n    accept = np.random.rand(n_stats) < eff\n    x_fit = x_true[accept]\n    w_fit = 1.0 / np.clip(efficiency(x_fit), 1e-3, 1.0) \n\n    tau_a, err_a = fit_asymptotic(x_fit, w_fit)\n    tau_d, err_d = fit_with_alpha(x_fit, w_fit)\n    tau_s, err_s = fit_weighted_hessian(x_fit, w_fit)\n    tau_b, err_b = fit_bootstrap(x_fit, w_fit)\n\n    pulls_alpha.append(pull(tau_d, tau_gen, err_d))\n    pulls_weighted_hessian.append(pull(tau_s, tau_gen, err_s))\n    pulls_bootstrap.append(pull(tau_b, tau_gen, err_b))\n    pulls_asymptotic.append(pull(tau_a, tau_gen, err_a))\n    \n\nfig, axs = plt.subplots(1, 4, figsize=(18, 5), sharey=True)\nmethods = ['Alpha', 'WeightedHessian', 'Bootstrap', 'Asymptotic']\npull_data = [pulls_alpha, pulls_weighted_hessian, pulls_bootstrap, pulls_asymptotic]\n\nfrom scipy.stats import norm \n\nfor i, (ax, pulls, label) in enumerate(zip(axs, pull_data, methods)):\n    ax.hist(pulls, bins=30, density=True, alpha=0.7)\n    mu, std = norm.fit(pulls)\n    x_vals = np.linspace(-2, 2, 500)\n    ax.plot(x_vals, norm.pdf(x_vals, mu, std), 'r--', label=f'mu={mu:.2f}, sigma={std:.2f}')\n    ax.set_title(f'{label}')\n    ax.set_xlabel('Pull')\n    ax.axvline(0, color='black', linestyle=':')\n    ax.legend()\n\nfig.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9cd41b36-896e-40c3-bacf-2b2e1d2ae79b","cell_type":"markdown","source":"Which uncertainty would you use in real analysis and why?","metadata":{}},{"id":"ff3d51cd-2ad0-49a0-b41e-56b6cf334afb","cell_type":"markdown","source":"/*Your answer*/ ","metadata":{}},{"id":"86df1245-c025-4844-a3b5-8a002fa2b292","cell_type":"markdown","source":"**If you want to save your results please make sure to download the jupyter notebook!**","metadata":{}}]}